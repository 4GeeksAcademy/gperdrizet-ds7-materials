# 2024-11-13: Wednesday, week 10

## 1. Plan

Tonight we will discuss the k nearest neighbors algorithm and use it to build a movie recommendation system. K nearest neighbors and the next topic, k-means, rely on the concepts of feature space and dimensionality and the notion of a distance metric. Below I have tried to provide some intuition about what those are.

## 2. Feature space

The range of features define a dataset's feature space. It is useful to use physical 3D space as an analogy for feature space. But, as we know features don't have to be physical dimensions, they can be anything! However, we can still think of an observation having a 'location' in feature space as defined by it's value for each feature. To see how this works, let's take a look at the classic *ires* dataset.

[3D Iris feature space plot](https://github.com/4GeeksAcademy/gperdrizet-ds7-materials/blob/main/how_to/visualize_feature_space.ipynb). **Note**: to see the plot, you have to run the notebooks in a codespace, it is an interactive visualization and can't be saved as an image.

This understanding also generalizes to numbers of dimensions higher than 3 - but obviously plotting becomes difficult!

## 3. Distance

If we accept the notion of each datapoint having a 'location' in high dimensional feature space, where each feature is an axis and the values are the coordinates of that point, then the concept of distance between points in feature space follows intuitively. One of several distance metrics can be used to calculate and compare how far apart data points are. In fact, this is a large part of the magic behind modern large language models - they embed text into feature space such that sentences or words and phrases that we would recognize as conceptually similar, end up 'near' each other in feature space.

Distance in feature space has many applications in data science and machine learning and is a huge topic.
